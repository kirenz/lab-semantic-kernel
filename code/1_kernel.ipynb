{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Semantic Kernel\n",
                "\n",
                "Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with programming languages like Python. \n",
                "\n",
                "By doing so, you can create AI apps that combine the best of both worlds.\n",
                "\n",
                "\n",
                "# Copilot from Microsoft\n",
                "\n",
                "## Satya Nadella at Microsoft Build 2023 about Copilot\n",
                "\n",
                "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/FaV0tIaWWEg?si=umcUKV_k4MM2oApI&amp;start=515\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
                "\n",
                "## Microsoft Copilot\n",
                "\n",
                "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/E5g20qmeKpg?si=oe9X-bNx_JYv8vQ0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
                "\n",
                "## Semantic Kernel is at the center of the copilot stack\n",
                "\n",
                "![](/images/sk.png)\n",
                "\n",
                "- To help developers build their own Copilot experiences on top of AI plugins, Microsoft has released Semantic Kernel, a lightweight open-source SDK that allows you to orchestrate AI plugins.\n",
                "\n",
                "- With Semantic Kernel, you can leverage the same AI orchestration patterns that power Microsoft 365 Copilot and Bing in your own apps, while still leveraging your existing development skills and investments.\n",
                "\n",
                "## Who needs Semantic Kernel?\n",
                "\n",
                "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/d9uzkkWfVho?si=ZWqPHqTyRVjg9E3w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
                "\n",
                "## Orchestration overview\n",
                "\n",
                "![](/images/orchestration.png)\n",
                "\n",
                "## Orchestration explained {.smaller}\n",
                "\n",
                "| Step | Component | Description |\n",
                "| --- | --- | --- |\n",
                "| 1 | Ask | It starts with a goal being sent to Semantic Kernel by either a user or developer. |\n",
                "| 2 | Kernel | The [kernel](https://learn.microsoft.com/en-us/semantic-kernel/create-chains/kernel) orchestrates a user's ask. To do so, the kernel runs a [pipeline / chain](https://learn.microsoft.com/en-us/semantic-kernel/create-chains/) that is defined by a developer. While the chain is run, a common context is provided by the kernel so data can be shared between functions. |\n",
                "| 2.1 | Memories | With a specialized plugin, a developer can recall and store context in vector databases. This allows developers to simulate [memory](https://learn.microsoft.com/en-us/semantic-kernel/memories/) within their AI apps. |\n",
                "| 2.2 | Planner | Developers can ask Semantic Kernel to auto create chains to address novel needs for a user. [Planner](https://learn.microsoft.com/en-us/semantic-kernel/create-chains/planner) achieves this by mixing-and-matching plugins that have already been loaded into the kernel to create additional steps. This is similar to how ChatGPT, Bing, and Microsoft 365 Copilot combines plugins together in their experiences. |\n",
                "| 2.3 | Connectors | To get additional data or to perform autonomous actions, you can use out-of-the-box plugins like the Microsoft Graph Connector kit or create a custom connector to provide data to your own services. |\n",
                "| 2.4 | Custom plugins | As a developer, you can create custom plugins that run inside of Semantic Kernel. These plugins can consist of either LLM prompts (semantic functions) or native C# or Python code (native function). This allows you to add new AI capabilities and integrate your existing apps and services into Semantic Kernel. |\n",
                "| 3 | Response | Once the kernel is done, you can send the response back to the user to let them know the process is complete. |\n",
                "\n",
                "[Source: Microsoft](https://learn.microsoft.com/en-us/semantic-kernel/overview/#seeing-ai-orchestration-with-semantic-kernel)\n",
                "\n",
                "\n",
                "# Create Kernels\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion\n",
                "import semantic_kernel as sk\n",
                "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Kernel"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "kernel = sk.Kernel()\n",
                "\n",
                "api_key, org_id = sk.openai_settings_from_dot_env()\n",
                "\n",
                "kernel.add_text_completion_service(\n",
                "    \"openai\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## AzureOpenAI (optional)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# kernel = sk.Kernel()\n",
                "\n",
                "# deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
                "\n",
                "# kernel.add_text_completion_service(\n",
                "#         \"azureopenai\", AzureChatCompletion(deployment, endpoint, api_key))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## HuggingFace Kernel"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "kernel = sk.Kernel()\n",
                "\n",
                "kernel.add_text_completion_service(\n",
                "    \"huggingface\", HuggingFaceTextCompletion(\"gpt2\", task=\"text-generation\"))\n",
                "\n",
                "\n",
                "print(\"You made an open source kernel using an open source AI model!\")"
            ],
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}